{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install translators"]},{"cell_type":"markdown","metadata":{},"source":["# This notebook was run on Kaggle usigng the data flickr8k for image captioning: \n","# https://www.kaggle.com/datasets/adityajn105/flickr8k"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f48ec521-cf45-4fcc-b266-16de6ca56488","_uuid":"b60eed4a-4184-49f4-b1fb-af2c50754b1e","execution":{"iopub.execute_input":"2022-05-20T13:04:36.712164Z","iopub.status.busy":"2022-05-20T13:04:36.711828Z","iopub.status.idle":"2022-05-20T13:04:39.449504Z","shell.execute_reply":"2022-05-20T13:04:39.448707Z","shell.execute_reply.started":"2022-05-20T13:04:36.712131Z"},"trusted":true},"outputs":[],"source":["data_location =  \"../input/flickr8k\"\n","\n","#imports\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","from torch.utils.data import DataLoader,Dataset\n","import torchvision.transforms as T\n","from torch.nn.utils.rnn import pad_sequence\n","\n","\n","#imports \n","import os\n","from collections import Counter\n","import spacy\n","\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","from time import time\n","\n","import translators as ts"]},{"cell_type":"markdown","metadata":{},"source":["### 1) Dataloader:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T13:04:39.451661Z","iopub.status.busy":"2022-05-20T13:04:39.451289Z","iopub.status.idle":"2022-05-20T13:04:40.380999Z","shell.execute_reply":"2022-05-20T13:04:40.380127Z","shell.execute_reply.started":"2022-05-20T13:04:39.45163Z"},"trusted":true},"outputs":[],"source":["spacy_eng = spacy.load(\"en\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T13:04:40.382888Z","iopub.status.busy":"2022-05-20T13:04:40.382533Z","iopub.status.idle":"2022-05-20T13:04:40.396979Z","shell.execute_reply":"2022-05-20T13:04:40.395798Z","shell.execute_reply.started":"2022-05-20T13:04:40.382851Z"},"trusted":true},"outputs":[],"source":["class Vocabulary:\n","    def __init__(self,freq_threshold):\n","        #setting the pre-reserved tokens int to string tokens\n","        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n","        \n","        #string to int tokens\n","        #its reverse dict self.itos\n","        self.stoi = {v:k for k,v in self.itos.items()}\n","        \n","        self.freq_threshold = freq_threshold\n","        \n","    def __len__(self): \n","        return len(self.itos)\n","    \n","    @staticmethod\n","    def tokenize(text):\n","        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n","    \n","    def build_vocab(self, sentence_list):\n","        frequencies = Counter()\n","        idx = 4\n","        \n","        for sentence in sentence_list:\n","            for word in self.tokenize(sentence):\n","                frequencies[word] += 1\n","                \n","                #add the word to the vocab if it reaches minum frequecy threshold\n","                if frequencies[word] == self.freq_threshold:\n","                    self.stoi[word] = idx\n","                    self.itos[idx] = word\n","                    idx += 1\n","    \n","    def numericalize(self,text):\n","        \"\"\" For each word in the text corresponding index token for that word form the vocab built as list \"\"\"\n","        tokenized_text = self.tokenize(text)\n","        return [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text ]    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T13:05:18.84546Z","iopub.status.busy":"2022-05-20T13:05:18.845112Z","iopub.status.idle":"2022-05-20T13:05:18.856045Z","shell.execute_reply":"2022-05-20T13:05:18.855194Z","shell.execute_reply.started":"2022-05-20T13:05:18.845428Z"},"trusted":true},"outputs":[],"source":["class FlickrDataset(Dataset):\n","    \"\"\"\n","    FlickrDataset\n","    \"\"\"\n","    def __init__(self,root_dir,caption_file,transform=None,freq_threshold=5):\n","        self.root_dir = root_dir\n","        self.df = pd.read_csv(caption_file)\n","        self.transform = transform\n","        \n","        #Get image and caption colum from the dataframe\n","        self.imgs = self.df[\"image\"]\n","        self.captions = self.df[\"caption\"]\n","        \n","        #Initialize vocabulary and build vocab\n","        self.vocab = Vocabulary(freq_threshold)\n","        self.vocab.build_vocab(self.captions.tolist())\n","        \n","    \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self,idx):\n","        caption = self.captions[idx]\n","        img_name = self.imgs[idx]\n","        img_location = os.path.join(self.root_dir,img_name)\n","        img = Image.open(img_location).convert(\"RGB\")\n","        \n","        #apply the transfromation to the image\n","        if self.transform is not None:\n","            img = self.transform(img)\n","        \n","        #numericalize the caption text\n","        caption_vec = [self.vocab.stoi[\"<SOS>\"]] + self.vocab.numericalize(caption) + [self.vocab.stoi[\"<EOS>\"]]\n","        \n","        return img, torch.tensor(caption_vec)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T13:05:19.602467Z","iopub.status.busy":"2022-05-20T13:05:19.602134Z","iopub.status.idle":"2022-05-20T13:05:19.609906Z","shell.execute_reply":"2022-05-20T13:05:19.608802Z","shell.execute_reply.started":"2022-05-20T13:05:19.602436Z"},"trusted":true},"outputs":[],"source":["class CapsCollate:\n","    \"\"\"\n","    Collate to apply the padding to the captions with dataloader\n","    \"\"\"\n","    def __init__(self,pad_idx,batch_first=False):\n","        self.pad_idx = pad_idx\n","        self.batch_first = batch_first\n","    \n","    def __call__(self,batch):\n","        imgs = [item[0].unsqueeze(0) for item in batch]\n","        imgs = torch.cat(imgs,dim=0)\n","        \n","        targets = [item[1] for item in batch]\n","        targets = pad_sequence(targets, batch_first=self.batch_first, padding_value=self.pad_idx)\n","        return imgs,targets"]},{"cell_type":"markdown","metadata":{"trusted":true},"source":["### 2) **<b>Implementing the Helper function to plot the Tensor image**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T13:05:21.727933Z","iopub.status.busy":"2022-05-20T13:05:21.727588Z","iopub.status.idle":"2022-05-20T13:05:21.736041Z","shell.execute_reply":"2022-05-20T13:05:21.735053Z","shell.execute_reply.started":"2022-05-20T13:05:21.727901Z"},"trusted":true},"outputs":[],"source":["#show the tensor image\n","import matplotlib.pyplot as plt\n","def show_image(img, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    \n","    #unnormalize \n","    img[0] = img[0] * 0.229\n","    img[1] = img[1] * 0.224 \n","    img[2] = img[2] * 0.225 \n","    img[0] += 0.485 \n","    img[1] += 0.456 \n","    img[2] += 0.406\n","    \n","    img = img.numpy().transpose((1, 2, 0))\n","    \n","    \n","    plt.imshow(img)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # pause a bit so that plots are updated"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T13:05:22.324347Z","iopub.status.busy":"2022-05-20T13:05:22.32403Z","iopub.status.idle":"2022-05-20T13:05:24.271253Z","shell.execute_reply":"2022-05-20T13:05:24.270402Z","shell.execute_reply.started":"2022-05-20T13:05:22.324305Z"},"trusted":true},"outputs":[],"source":["#Initiate the Dataset and Dataloader\n","\n","#setting the constants\n","data_location =  \"../input/flickr8k\"\n","BATCH_SIZE = 256\n","# BATCH_SIZE = 6\n","NUM_WORKER = 4\n","\n","#defining the transform to be applied\n","transforms = T.Compose([\n","    T.Resize(226),                     \n","    T.RandomCrop(224),                 \n","    T.ToTensor(),                               \n","    T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n","])\n","\n","\n","#testing the dataset class\n","dataset =  FlickrDataset(\n","    root_dir = data_location+\"/Images\",\n","    caption_file = data_location+\"/captions.txt\",\n","    transform=transforms\n",")\n","\n","#writing the dataloader\n","pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n","\n","data_loader = DataLoader(\n","    dataset=dataset,\n","    batch_size=BATCH_SIZE,\n","    num_workers=NUM_WORKER,\n","    shuffle=True,\n","    collate_fn=CapsCollate(pad_idx=pad_idx,batch_first=True)\n",")\n","\n","#vocab_size\n","vocab_size = len(dataset.vocab)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"markdown","metadata":{"trusted":true},"source":["### 3) Defining the Model Architecture\n","\n","Model is seq2seq model. In the **encoder** pretrained ResNet model is used to extract the features. Decoder, is the implementation of the Bahdanau Attention Decoder. In the decoder model **LSTM cell**."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T13:05:24.459535Z","iopub.status.busy":"2022-05-20T13:05:24.459222Z","iopub.status.idle":"2022-05-20T13:05:24.464817Z","shell.execute_reply":"2022-05-20T13:05:24.464002Z","shell.execute_reply.started":"2022-05-20T13:05:24.459505Z"},"trusted":true},"outputs":[],"source":["import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import torchvision.models as models\n","from torch.utils.data import DataLoader,Dataset\n","import torchvision.transforms as T"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T13:05:25.422267Z","iopub.status.busy":"2022-05-20T13:05:25.421956Z","iopub.status.idle":"2022-05-20T13:05:25.430897Z","shell.execute_reply":"2022-05-20T13:05:25.430063Z","shell.execute_reply.started":"2022-05-20T13:05:25.422237Z"},"trusted":true},"outputs":[],"source":["class EncoderCNN(nn.Module):\n","    def __init__(self):\n","        super(EncoderCNN, self).__init__()\n","        resnet = models.resnet50(pretrained=True)\n","        for param in resnet.parameters():\n","            param.requires_grad_(False)\n","        \n","        modules = list(resnet.children())[:-2]\n","        self.resnet = nn.Sequential(*modules)\n","        \n","\n","    def forward(self, images):\n","        features = self.resnet(images)                                    #(batch_size,2048,7,7)\n","        features = features.permute(0, 2, 3, 1)                           #(batch_size,7,7,2048)\n","        features = features.view(features.size(0), -1, features.size(-1)) #(batch_size,49,2048)\n","        return features\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T13:05:26.085918Z","iopub.status.busy":"2022-05-20T13:05:26.085602Z","iopub.status.idle":"2022-05-20T13:05:26.095259Z","shell.execute_reply":"2022-05-20T13:05:26.094388Z","shell.execute_reply.started":"2022-05-20T13:05:26.085887Z"},"trusted":true},"outputs":[],"source":["#Bahdanau Attention\n","class Attention(nn.Module):\n","    def __init__(self, encoder_dim,decoder_dim,attention_dim):\n","        super(Attention, self).__init__()\n","        \n","        self.attention_dim = attention_dim\n","        \n","        self.W = nn.Linear(decoder_dim,attention_dim)\n","        self.U = nn.Linear(encoder_dim,attention_dim)\n","        \n","        self.A = nn.Linear(attention_dim,1)\n","        \n","        \n","        \n","        \n","    def forward(self, features, hidden_state):\n","        u_hs = self.U(features)     #(batch_size,num_layers,attention_dim)\n","        w_ah = self.W(hidden_state) #(batch_size,attention_dim)\n","        \n","        combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1)) #(batch_size,num_layers,attemtion_dim)\n","        \n","        attention_scores = self.A(combined_states)         #(batch_size,num_layers,1)\n","        attention_scores = attention_scores.squeeze(2)     #(batch_size,num_layers)\n","        \n","        \n","        alpha = F.softmax(attention_scores,dim=1)          #(batch_size,num_layers)\n","        \n","        attention_weights = features * alpha.unsqueeze(2)  #(batch_size,num_layers,features_dim)\n","        attention_weights = attention_weights.sum(dim=1)   #(batch_size,num_layers)\n","        \n","        return alpha,attention_weights\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T13:05:26.501641Z","iopub.status.busy":"2022-05-20T13:05:26.501297Z","iopub.status.idle":"2022-05-20T13:05:26.617522Z","shell.execute_reply":"2022-05-20T13:05:26.616625Z","shell.execute_reply.started":"2022-05-20T13:05:26.501611Z"},"trusted":true},"outputs":[],"source":["#Attention Decoder\n","class DecoderRNN(nn.Module):\n","    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n","        super().__init__()\n","        \n","        #save the model param\n","        self.vocab_size = vocab_size\n","        self.attention_dim = attention_dim\n","        self.decoder_dim = decoder_dim\n","        \n","        self.embedding = nn.Embedding(vocab_size,embed_size)\n","        self.attention = Attention(encoder_dim,decoder_dim,attention_dim)\n","        \n","        \n","        self.init_h = nn.Linear(encoder_dim, decoder_dim)  \n","        self.init_c = nn.Linear(encoder_dim, decoder_dim)  \n","        self.lstm_cell = nn.LSTMCell(embed_size+encoder_dim,decoder_dim,bias=True)\n","        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n","        \n","        \n","        self.fcn = nn.Linear(decoder_dim,vocab_size)\n","        self.drop = nn.Dropout(drop_prob)\n","        \n","        \n","    \n","    def forward(self, features, captions):\n","        \n","        #vectorize the caption\n","        embeds = self.embedding(captions)\n","        \n","        # Initialize LSTM state\n","        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n","        \n","        #get the seq length to iterate\n","        seq_length = len(captions[0])-1 #Exclude the last one\n","        batch_size = captions.size(0)\n","        num_features = features.size(1)\n","        \n","        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(device)\n","        alphas = torch.zeros(batch_size, seq_length,num_features).to(device)\n","                \n","        for s in range(seq_length):\n","            alpha,context = self.attention(features, h)\n","            lstm_input = torch.cat((embeds[:, s], context), dim=1)\n","            h, c = self.lstm_cell(lstm_input, (h, c))\n","                    \n","            output = self.fcn(self.drop(h))\n","            \n","            preds[:,s] = output\n","            alphas[:,s] = alpha  \n","        \n","        \n","        return preds, alphas\n","    \n","    def generate_caption(self,features,max_len=20,vocab=None):\n","        # Inference part\n","        # Given the image features generate the captions\n","        \n","        batch_size = features.size(0)\n","        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n","        \n","        alphas = []\n","        \n","        #starting input\n","        word = torch.tensor(vocab.stoi['<SOS>']).view(1,-1).to(device)\n","        embeds = self.embedding(word)\n","\n","        \n","        captions = []\n","        \n","        for i in range(max_len):\n","            alpha,context = self.attention(features, h)\n","            \n","            \n","            #store the apla score\n","            alphas.append(alpha.cpu().detach().numpy())\n","            \n","            lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n","            h, c = self.lstm_cell(lstm_input, (h, c))\n","            output = self.fcn(self.drop(h))\n","            output = output.view(batch_size,-1)\n","        \n","            \n","            #select the word with most val\n","            predicted_word_idx = output.argmax(dim=1)\n","            \n","            #save the generated word\n","            captions.append(predicted_word_idx.item())\n","            \n","            #end if <EOS detected>\n","            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n","                break\n","            \n","            #send generated word as the next caption\n","            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n","        \n","        #covert the vocab idx to words and return sentence\n","        return [vocab.itos[idx] for idx in captions],alphas\n","    \n","    \n","    def init_hidden_state(self, encoder_out):\n","        mean_encoder_out = encoder_out.mean(dim=1)\n","        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n","        c = self.init_c(mean_encoder_out)\n","        return h, c\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T13:05:26.989963Z","iopub.status.busy":"2022-05-20T13:05:26.989616Z","iopub.status.idle":"2022-05-20T13:05:26.997251Z","shell.execute_reply":"2022-05-20T13:05:26.996314Z","shell.execute_reply.started":"2022-05-20T13:05:26.98993Z"},"trusted":true},"outputs":[],"source":["class EncoderDecoder(nn.Module):\n","    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n","        super().__init__()\n","        self.encoder = EncoderCNN()\n","        self.decoder = DecoderRNN(\n","            embed_size=embed_size,\n","            vocab_size = len(dataset.vocab),\n","            attention_dim=attention_dim,\n","            encoder_dim=encoder_dim,\n","            decoder_dim=decoder_dim\n","        )\n","        \n","    def forward(self, images, captions):\n","        features = self.encoder(images)\n","        outputs = self.decoder(features, captions)\n","        return outputs\n"]},{"cell_type":"markdown","metadata":{},"source":["### 4) Setting Hypperparameter and Init the model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T13:05:30.815806Z","iopub.status.busy":"2022-05-20T13:05:30.815487Z","iopub.status.idle":"2022-05-20T13:05:30.821402Z","shell.execute_reply":"2022-05-20T13:05:30.820432Z","shell.execute_reply.started":"2022-05-20T13:05:30.815776Z"},"trusted":true},"outputs":[],"source":["#Hyperparams\n","embed_size=300\n","vocab_size = len(dataset.vocab)\n","attention_dim=256\n","encoder_dim=2048\n","decoder_dim=512\n","learning_rate = 3e-4\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T13:05:32.796734Z","iopub.status.busy":"2022-05-20T13:05:32.796415Z","iopub.status.idle":"2022-05-20T13:05:48.109737Z","shell.execute_reply":"2022-05-20T13:05:48.108857Z","shell.execute_reply.started":"2022-05-20T13:05:32.796704Z"},"trusted":true},"outputs":[],"source":["#init model\n","model = EncoderDecoder(\n","    embed_size=300,\n","    vocab_size = len(dataset.vocab),\n","    attention_dim=256,\n","    encoder_dim=2048,\n","    decoder_dim=512\n",").to(device)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T13:05:48.113598Z","iopub.status.busy":"2022-05-20T13:05:48.113333Z","iopub.status.idle":"2022-05-20T13:05:48.119963Z","shell.execute_reply":"2022-05-20T13:05:48.119088Z","shell.execute_reply.started":"2022-05-20T13:05:48.113571Z"},"trusted":true},"outputs":[],"source":["#helper function to save the model\n","def save_model(model,num_epochs):\n","    model_state = {\n","        'num_epochs':num_epochs,\n","        'embed_size':embed_size,\n","        'vocab_size':len(dataset.vocab),\n","        'attention_dim':attention_dim,\n","        'encoder_dim':encoder_dim,\n","        'decoder_dim':decoder_dim,\n","        'state_dict':model.state_dict()\n","    }\n","\n","    torch.save(model_state,'attention_model_state.pth')"]},{"cell_type":"markdown","metadata":{},"source":["## 5) Training Job from above configs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-19T23:38:07.499566Z","iopub.status.busy":"2022-05-19T23:38:07.499235Z","iopub.status.idle":"2022-05-20T01:17:49.453381Z","shell.execute_reply":"2022-05-20T01:17:49.445816Z","shell.execute_reply.started":"2022-05-19T23:38:07.499531Z"},"trusted":true},"outputs":[],"source":["num_epochs = 25\n","print_every = 16\n","\n","s_time = time()\n","\n","losses = []\n","\n","for epoch in range(1,num_epochs+1):   \n","    for idx, (image, captions) in enumerate(iter(data_loader)):\n","        image,captions = image.to(device),captions.to(device)\n","\n","        # Zero the gradients.\n","        optimizer.zero_grad()\n","\n","        # Feed forward\n","        outputs,attentions = model(image, captions)\n","\n","        # Calculate the batch loss.\n","        targets = captions[:,1:]\n","        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n","        \n","        # Backward pass.\n","        loss.backward()\n","        losses.append(loss.item())\n","\n","        # Update the parameters in the optimizer.\n","        optimizer.step()\n","\n","        if (idx+1)%print_every == 0:\n","            print(f\"Epoch: {epoch} Step: {idx}/{len(data_loader)} loss: {loss.item():.5f} time: {time()-s_time:.3f}\".format(epoch, loss.item(),time()-s_time))\n","            s_time = time()\n","            \n","    model.eval()\n","    with torch.no_grad():\n","        dataiter = iter(data_loader)\n","        img,_ = next(dataiter)\n","        features = model.encoder(img[0:1].to(device))\n","        caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n","        caption = ' '.join(caps)\n","        show_image(img[0],title=caption)\n","\n","    model.train()\n","        \n","    #save the latest model\n","    save_model(model,epoch)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T01:18:09.905828Z","iopub.status.busy":"2022-05-20T01:18:09.905478Z","iopub.status.idle":"2022-05-20T01:18:10.221723Z","shell.execute_reply":"2022-05-20T01:18:10.220665Z","shell.execute_reply.started":"2022-05-20T01:18:09.905791Z"},"trusted":true},"outputs":[],"source":["save_model(model,epoch)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T13:06:55.417187Z","iopub.status.busy":"2022-05-20T13:06:55.416874Z","iopub.status.idle":"2022-05-20T13:06:55.528333Z","shell.execute_reply":"2022-05-20T13:06:55.52742Z","shell.execute_reply.started":"2022-05-20T13:06:55.417159Z"},"trusted":true},"outputs":[],"source":["saved_cp = torch.load('../../models/caption/attention_model_state.pt', map_location=device)\n","model.load_state_dict(saved_cp['state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T13:08:00.146028Z","iopub.status.busy":"2022-05-20T13:08:00.145568Z","iopub.status.idle":"2022-05-20T13:08:14.054464Z","shell.execute_reply":"2022-05-20T13:08:14.053401Z","shell.execute_reply.started":"2022-05-20T13:08:00.145983Z"},"trusted":true},"outputs":[],"source":["model.eval()\n","with torch.no_grad():\n","    img,_ = next(iter(data_loader))\n","    features = model.encoder(img[0:1].to(device))\n","    caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n","    caption = ' '.join(caps)\n","    show_image(img[0],title=caption)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T13:28:15.553163Z","iopub.status.busy":"2022-05-20T13:28:15.552843Z","iopub.status.idle":"2022-05-20T13:28:38.972581Z","shell.execute_reply":"2022-05-20T13:28:38.971682Z","shell.execute_reply.started":"2022-05-20T13:28:15.553132Z"},"trusted":true},"outputs":[],"source":["!pip install translators"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T13:30:05.63074Z","iopub.status.busy":"2022-05-20T13:30:05.630404Z","iopub.status.idle":"2022-05-20T13:30:08.881401Z","shell.execute_reply":"2022-05-20T13:30:08.880314Z","shell.execute_reply.started":"2022-05-20T13:30:05.630709Z"},"trusted":true},"outputs":[],"source":["phrase = 'The quick brown fox jumps over the lazy dog.'\n","ts.google(phrase, from_language='en', to_language='ar')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-20T13:32:08.335917Z","iopub.status.busy":"2022-05-20T13:32:08.335513Z","iopub.status.idle":"2022-05-20T13:32:25.255552Z","shell.execute_reply":"2022-05-20T13:32:25.254794Z","shell.execute_reply.started":"2022-05-20T13:32:08.335877Z"},"trusted":true},"outputs":[],"source":["model.eval()\n","with torch.no_grad():\n","    img,_ = next(iter(data_loader))\n","    features = model.encoder(img[0:1].to(device))\n","    caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n","    caption = ' '.join(caps[:-2])\n","    print(ts.google(caption, from_language='en', to_language='ar'))\n","    show_image(img[0],title=caption)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
