{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T05:22:57.162327Z","iopub.status.busy":"2022-05-28T05:22:57.161668Z","iopub.status.idle":"2022-05-28T05:22:57.174857Z","shell.execute_reply":"2022-05-28T05:22:57.173701Z","shell.execute_reply.started":"2022-05-28T05:22:57.162285Z"},"trusted":true},"outputs":[],"source":["from io import open\n","import random\n","import time\n","import math\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.utils.data as Data\n","from torch import optim\n","import torch.nn.functional as F\n","from helper import *\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader, Dataset\n","\n","import pandas as pd\n","import numpy as np\n","import random\n","import math\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from time import time\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T05:22:57.177208Z","iopub.status.busy":"2022-05-28T05:22:57.176703Z","iopub.status.idle":"2022-05-28T05:22:57.562625Z","shell.execute_reply":"2022-05-28T05:22:57.561420Z","shell.execute_reply.started":"2022-05-28T05:22:57.177168Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv('../input/arabic-poem/aroud_dataset.csv')"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T05:22:57.565449Z","iopub.status.busy":"2022-05-28T05:22:57.564722Z","iopub.status.idle":"2022-05-28T05:22:57.572608Z","shell.execute_reply":"2022-05-28T05:22:57.571259Z","shell.execute_reply.started":"2022-05-28T05:22:57.565405Z"},"trusted":true},"outputs":[],"source":["use_cuda = torch.cuda.is_available()\n","SOS_token = 0\n","EOS_token = 1\n","PAD_token = 2\n","MAX_LENGTH = 15\n","teacher_forcing_ratio = 0.5\n","hidden_size = 29 #a-z+SOS+EOS+PAD\n","batch_size = 128\n","epochs = 100"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T05:22:57.574956Z","iopub.status.busy":"2022-05-28T05:22:57.574472Z","iopub.status.idle":"2022-05-28T05:22:57.587625Z","shell.execute_reply":"2022-05-28T05:22:57.586607Z","shell.execute_reply.started":"2022-05-28T05:22:57.574917Z"},"trusted":true},"outputs":[],"source":["special = ['َ', 'ُ', 'ِ', 'ً', 'ٌ', 'ٍ', 'َّ', 'ُّ', 'ِّ','ًّ', 'ٌّ', 'ٍّ', 'ْ','ّ']\n","vocab = ['SOS', 'EOS', 'PAD', ' ','ء', 'آ', 'أ', 'ؤ', 'إ', 'ئ', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ى', 'ي'] \n","# wazn_vocab = ['SOS', 'EOS', ' ', 'ا', 'ت', 'س', 'ع','ف', 'ل', 'م', 'ن', 'و', 'ي'] + ['َ', 'ُ', 'ِ', 'ً', 'ٌ', 'ٍ', 'ْ','ّ']\n","\n","wazn_vocab = ['SOS', 'EOS', 'PAD', 'مُتْفَاعِلُنْ', 'مُفَاعَلْتُنْ', 'فَعُولُنْ', 'مُسْتَفْعِلُنْ', 'مَفَاعِيلُنْ', 'فَاعِلُنْ', 'مُفَاعَلَتُنْ', 'مَفْعُولُنْ', 'مَفْعُولَاتُ', 'فَاعِلَاتُنْ', 'فِعْلُنْ', 'فَعِلَاتُنْ', 'فَعِلُنْ', 'مُسْتَفْعِلَانْ', 'فَعُولْ', 'مَفَاعِلُنْ', 'مُتَعِلُنْ', 'فَعِلَاتُ', 'فَاعِلَاتُ', 'فَعُولُ', 'مُتَفْعِلُنْ', 'مُسْتَعِلُنْ', 'مُتَفَاعِلُنْ', 'مَفَاعِيلُ', 'مَفْعُلَاتُ']"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T05:22:57.592532Z","iopub.status.busy":"2022-05-28T05:22:57.591848Z","iopub.status.idle":"2022-05-28T05:22:57.604701Z","shell.execute_reply":"2022-05-28T05:22:57.603552Z","shell.execute_reply.started":"2022-05-28T05:22:57.592484Z"},"trusted":true},"outputs":[],"source":["class MyDataset:\n","    def __init__(self,data):\n","        self.data = data\n","    \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, item):\n","        line = self.data.iloc[item]\n","        \n","        x = line['line']\n","        y = line['wazn']\n","        \n","        x = get_raw_sentence(x)\n","        \n","        try:\n","            x_token = [vocab.index(c) for c in x]\n","            y_token = [wazn_vocab.index(c) for c in y.split(' ')]\n","        except:\n","            return self.__getitem__(0)\n","        \n","        x_tensor = torch.tensor(x_token, dtype=torch.int64)\n","        y_tensor = torch.tensor(y_token, dtype=torch.int64)        \n","        \n","        return {\n","            'x':x_tensor,\n","            'y':y_tensor,\n","        }"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T05:22:57.607242Z","iopub.status.busy":"2022-05-28T05:22:57.606676Z","iopub.status.idle":"2022-05-28T05:22:57.619251Z","shell.execute_reply":"2022-05-28T05:22:57.617962Z","shell.execute_reply.started":"2022-05-28T05:22:57.607200Z"},"trusted":true},"outputs":[],"source":["def get_raw_sentence(sentence):\n","    \"\"\"\n","    Get a raw sentence without tachkil\n","    \"\"\"\n","    \n","    sentence = list(sentence)\n","\n","    S = []\n","\n","    i, j = 0, 1\n","    L = len(sentence)\n","\n","    while i < L - 1:\n","        if sentence[i] == ' ':\n","            S.append(' ')\n","            i += 1\n","            j += 1\n","\n","        else:\n","            S.append(sentence[i])\n","            if sentence[j] not in special:\n","                i += 1\n","                j += 1\n","\n","            elif sentence[j] != 'ّ':\n","                i += 2\n","                j += 2\n","\n","            else:  # sentence[j] = 'ّ'\n","                if j == L-1 or sentence[j+1] not in special:\n","                    i += 2\n","                    j += 2\n","                else:\n","                    i += 3\n","                    j += 3\n","\n","    if i < L and not sentence[-1] in special:\n","        S.append(sentence[-1])\n","\n","    return S\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T05:22:57.621701Z","iopub.status.busy":"2022-05-28T05:22:57.620955Z","iopub.status.idle":"2022-05-28T05:22:57.635670Z","shell.execute_reply":"2022-05-28T05:22:57.634426Z","shell.execute_reply.started":"2022-05-28T05:22:57.621657Z"},"trusted":true},"outputs":[],"source":["def collate_fn(batch, x_maxlen=50, y_maxlen=4):\n","    inputs, targets = [], []\n","    \n","    for line in batch:\n","        inputs.append(line['x'])\n","        targets.append(line['y'])\n","\n","    \n","    T = inputs[0]\n","    inputs[0]  = nn.ConstantPad1d((0, x_maxlen - T.shape[0]), PAD_token)(T)\n","    T = targets[0]\n","    targets[0] = nn.ConstantPad1d((0, y_maxlen - T.shape[0]), PAD_token)(T)\n","        \n","    inputs    = pad_sequence(inputs,  padding_value=PAD_token , batch_first=True)\n","    targets   = pad_sequence(targets, padding_value=PAD_token , batch_first=True)\n","    \n","    return inputs, targets\n","\n","\n","def split_data(data, s=0.7):\n","    import random\n","    ids = list(range(len(data)))\n","    random.shuffle(ids)\n","    x = int(len(ids)*s)\n","    return data.iloc[ids[:x]], data.iloc[ids[x:]]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Model:"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T05:22:57.638561Z","iopub.status.busy":"2022-05-28T05:22:57.637657Z","iopub.status.idle":"2022-05-28T05:22:57.650894Z","shell.execute_reply":"2022-05-28T05:22:57.649889Z","shell.execute_reply.started":"2022-05-28T05:22:57.638486Z"},"trusted":true},"outputs":[],"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru1 = nn.GRU(hidden_size, hidden_size, bidirectional=True, batch_first=True)\n","        self.gru2 = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n","        self.gru3 = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n","        \n","    def forward(self, input, hidden1, hidden2, hidden3):\n","        embedded = self.embedding(input)\n","        output = embedded     \n","        \n","        output, hidden1 = self.gru1(output, hidden1)\n","        output, hidden2 = self.gru2(output, hidden2)\n","        output, hidden3 = self.gru3(output, hidden3)\n","        \n","        \n","        return output, hidden1, hidden2, hidden3\n","\n","    def initHidden(self, bs):\n","        return (torch.zeros(2, bs, self.hidden_size, device=device),\n","                torch.zeros(2, bs, self.hidden_size, device=device),\n","                   torch.zeros(2, bs, self.hidden_size, device=device))"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T05:22:57.653298Z","iopub.status.busy":"2022-05-28T05:22:57.652778Z","iopub.status.idle":"2022-05-28T05:22:57.668973Z","shell.execute_reply":"2022-05-28T05:22:57.667697Z","shell.execute_reply.started":"2022-05-28T05:22:57.653258Z"},"trusted":true},"outputs":[],"source":["class DecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size):\n","        super(DecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.gru1 = nn.GRU(hidden_size, hidden_size, bidirectional=True, batch_first=True)\n","        self.gru2 = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n","        self.gru3 = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n","        \n","        self.out = nn.Linear(hidden_size*2, output_size)\n","        self.softmax = nn.LogSoftmax(dim=-1)\n","\n","    def forward(self, input, hidden1, hidden2, hidden3):\n","        output = self.embedding(input)\n","        output = F.relu(output)\n","        \n","        output, hidden1 = self.gru1(output, hidden1)\n","        output, hidden2 = self.gru2(output, hidden2)\n","        output, hidden3 = self.gru3(output, hidden3)\n","                \n","#         output = self.softmax(self.out(output))\n","        output = self.out(output)\n","        return output, hidden1, hidden2, hidden3\n","\n","    def initHidden(self):\n","        return (torch.zeros(2, bs, self.hidden_size, device=device),\n","                torch.zeros(2, bs, self.hidden_size, device=device),\n","                torch.zeros(2, bs, self.hidden_size, device=device))"]},{"cell_type":"code","execution_count":89,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T05:45:46.277612Z","iopub.status.busy":"2022-05-28T05:45:46.277124Z","iopub.status.idle":"2022-05-28T05:45:46.487917Z","shell.execute_reply":"2022-05-28T05:45:46.486932Z","shell.execute_reply.started":"2022-05-28T05:45:46.277575Z"},"trusted":true},"outputs":[],"source":["train_data, val_data = split_data(data)\n","\n","train_dataset = MyDataset(train_data)\n","val_dataset   = MyDataset(val_data)\n","\n","f'There are {len(train_dataset) :,} samples for training, and {len(val_dataset) :,} samples for validation testing'"]},{"cell_type":"code","execution_count":90,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T05:45:49.037439Z","iopub.status.busy":"2022-05-28T05:45:49.037035Z","iopub.status.idle":"2022-05-28T05:45:49.046336Z","shell.execute_reply":"2022-05-28T05:45:49.043814Z","shell.execute_reply.started":"2022-05-28T05:45:49.037411Z"},"trusted":true},"outputs":[],"source":["TRAIN_BATCH_SIZE = 64\n","\n","train_dataloader = torch.utils.data.DataLoader(\n","    dataset=train_dataset,\n","    batch_size=TRAIN_BATCH_SIZE,\n","    shuffle=True, \n","    collate_fn=collate_fn, \n","    drop_last=True,\n","#     prefetch_factor=1, \n","#     num_workers=1\n","    )\n","\n","val_dataloader = torch.utils.data.DataLoader(\n","    dataset=val_dataset,\n","    batch_size=256 * 8 * 2,\n","    shuffle=False, \n","    collate_fn=collate_fn, \n","    drop_last=True,\n","#     prefetch_factor=1, \n","#     num_workers=1\n","    )"]},{"cell_type":"code","execution_count":91,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T05:45:51.498855Z","iopub.status.busy":"2022-05-28T05:45:51.498338Z","iopub.status.idle":"2022-05-28T05:45:51.611160Z","shell.execute_reply":"2022-05-28T05:45:51.610238Z","shell.execute_reply.started":"2022-05-28T05:45:51.498815Z"},"trusted":true},"outputs":[],"source":["encoder = EncoderRNN(len(vocab), 256).to(device)\n","decoder = DecoderRNN(256, len(wazn_vocab)).to(device)\n","\n","learning_rate = 0.0007\n","\n","encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n","decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n","\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":92,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T05:45:54.320972Z","iopub.status.busy":"2022-05-28T05:45:54.320217Z","iopub.status.idle":"2022-05-28T05:45:54.336043Z","shell.execute_reply":"2022-05-28T05:45:54.334960Z","shell.execute_reply.started":"2022-05-28T05:45:54.320934Z"},"trusted":true},"outputs":[],"source":["teacher_forcing_ratio = 0.5\n","\n","\n","def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=50, bs=64):\n","    \n","    encoder.train()\n","    decoder.train()\n","\n","    \n","    encoder_hidden1, encoder_hidden2, encoder_hidden3 = encoder.initHidden(bs)\n","\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    input_length = max_length\n","    target_length = 4\n","\n","    encoder_outputs = torch.zeros(max_length, bs, encoder.hidden_size * 2, device=device)\n","\n","    loss = 0\n","    \n","        \n","    for ei in range(input_length):\n","        encoder_output, encoder_hidden1, encoder_hidden2, encoder_hidden3 = encoder(\n","            input_tensor[:, :, ei], encoder_hidden1, encoder_hidden2, encoder_hidden3)\n","        \n","        encoder_outputs[ei] = encoder_output[:, 0]\n","\n","    \n","    \n","#     decoder_input = torch.tensor([[SOS_token]], device=device)\n","    decoder_input = torch.full((bs,1), SOS_token, device=device)\n","    decoder_hidden1, decoder_hidden2, decoder_hidden3 = encoder_hidden1, encoder_hidden2, encoder_hidden3\n","    \n","    \n","#     print(decoder_input.shape, decoder_hidden.shape)\n","\n","\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    if use_teacher_forcing:\n","        # Teacher forcing: Feed the target as the next input\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden1, decoder_hidden2, decoder_hidden3 = decoder(\n","                decoder_input, decoder_hidden1, decoder_hidden2, decoder_hidden3)\n","            \n","            loss += criterion(decoder_output.squeeze(1), target_tensor[:,di])\n","            decoder_input = target_tensor[:,di:di+1]  # Teacher forcing\n","        \n","    else:\n","        # Without teacher forcing: use its own predictions as the next input\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden1, decoder_hidden2, decoder_hidden3 = decoder(\n","                decoder_input, decoder_hidden1, decoder_hidden2, decoder_hidden3)\n","            \n","            topv, topi = decoder_output.topk(1)\n","            decoder_input = topi.squeeze().detach()  # detach from history as input\n","                        \n","            loss += criterion(decoder_output.squeeze(1), target_tensor[:,di])\n","            \n","            decoder_input = decoder_input.unsqueeze(-1)\n","\n","    loss.backward()\n","\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","\n","    return loss.item() / target_length"]},{"cell_type":"code","execution_count":102,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T06:18:13.685431Z","iopub.status.busy":"2022-05-28T06:18:13.685051Z","iopub.status.idle":"2022-05-28T06:18:13.700020Z","shell.execute_reply":"2022-05-28T06:18:13.698809Z","shell.execute_reply.started":"2022-05-28T06:18:13.685402Z"},"trusted":true},"outputs":[],"source":["def eval_accuracy(bs):\n","\n","    max_length= 50\n","\n","    sums = 0\n","    vs = 0\n","\n","    for i, (input_tensor, target_tensor) in enumerate(val_dataloader):\n","        \n","        \n","\n","        encoder.eval()\n","        decoder.eval()\n","\n","        input_tensor = input_tensor.unsqueeze(1).to(device)\n","\n","\n","        encoder_hidden1, encoder_hidden2, encoder_hidden3 = encoder.initHidden(bs)\n","\n","        input_length = max_length\n","        target_length = 4\n","\n","        encoder_outputs = torch.zeros(max_length, bs, encoder.hidden_size * 2, device=device)\n","\n","        loss = 0\n","        \n","#         print(input_tensor.shape)\n","\n","        for ei in range(input_length):\n","            encoder_output, encoder_hidden1, encoder_hidden2, encoder_hidden3 = encoder(\n","                input_tensor[:, :, ei], encoder_hidden1, encoder_hidden2, encoder_hidden3)\n","\n","            encoder_outputs[ei] = encoder_output[:, 0]\n","\n","\n","\n","    #     decoder_input = torch.tensor([[SOS_token]], device=device)\n","        decoder_input = torch.full((bs,1), SOS_token, device=device)\n","        decoder_hidden1, decoder_hidden2, decoder_hidden3 = encoder_hidden1, encoder_hidden2, encoder_hidden3\n","\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden1, decoder_hidden2, decoder_hidden3 = decoder(\n","                decoder_input, decoder_hidden1, decoder_hidden2, decoder_hidden3)\n","\n","            topv, topi = decoder_output.topk(1)\n","            decoder_input = topi.squeeze().detach()  # detach from history as input\n","\n","    #         print(torch.argmax(decoder_output, dim=-1).shape)\n","    #         print(target_tensor[:,di].shape)\n","\n","            m = torch.argmax(decoder_output, dim=-1).cpu().view(-1)\n","            v = target_tensor[:,di]\n","\n","    #         print(len(v))\n","            m = m[v != PAD_token]\n","            v = v[v != PAD_token]\n","            \n","            try:\n","                sums += sum(m == v).item()\n","                vs += len(v)\n","            except:\n","                pass\n","\n","\n","            decoder_input = decoder_input.unsqueeze(-1)\n","        break\n","    return sums/vs"]},{"cell_type":"code","execution_count":104,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T06:29:13.065688Z","iopub.status.busy":"2022-05-28T06:29:13.065252Z","iopub.status.idle":"2022-05-28T06:29:13.070779Z","shell.execute_reply":"2022-05-28T06:29:13.069572Z","shell.execute_reply.started":"2022-05-28T06:29:13.065641Z"},"trusted":true},"outputs":[],"source":["best_acc = 0"]},{"cell_type":"code","execution_count":105,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T06:29:13.671496Z","iopub.status.busy":"2022-05-28T06:29:13.671001Z","iopub.status.idle":"2022-05-28T06:40:54.039113Z","shell.execute_reply":"2022-05-28T06:40:54.037363Z","shell.execute_reply.started":"2022-05-28T06:29:13.671463Z"},"trusted":true},"outputs":[],"source":["print_loss_total = 0  # Reset every print_every\n","print_every = 200\n","\n","for epoch in range(5,40):\n","\n","    \n","    start = time()\n","    for i, (input_tensor, target_tensor) in enumerate(train_dataloader):\n","        \n","        input_tensor  = input_tensor.to(device)\n","        input_tensor = input_tensor.unsqueeze(1)\n","        \n","        target_tensor = target_tensor.to(device)\n","#         target_tensor = target_tensor.unsqueeze(1)\n","        \n","        \n","        loss = train(input_tensor, target_tensor, encoder,\n","                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n","        \n","        print_loss_total += loss\n","\n","        if (i+1) % print_every == 0:\n","            print_loss_avg = print_loss_total / print_every\n","            print_loss_total = 0\n","            print(f' EPOCH:{epoch}    STEP:{i}/{len(train_dataloader)}    AVG LOSS:{print_loss_avg}    TIME: {time()-start:.2f}')\n","            start = time()\n","    \n","    acc = eval_accuracy(256 * 8 * 2) \n","    print(f'VAL ACCCCCCC: {acc}')\n","\n","    if acc > best_acc:\n","        print('BEST MODEL YAY')\n","        best_acc = acc\n","        torch.save(encoder.state_dict(), f'encoder512.pt')\n","        torch.save(decoder.state_dict(), f'decoder512.pt')\n","        \n","    \n","    print('\\n\\n')"]},{"cell_type":"code","execution_count":98,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T05:53:01.934018Z","iopub.status.busy":"2022-05-28T05:53:01.933624Z","iopub.status.idle":"2022-05-28T05:53:03.254188Z","shell.execute_reply":"2022-05-28T05:53:03.252969Z","shell.execute_reply.started":"2022-05-28T05:53:01.933989Z"},"trusted":true},"outputs":[],"source":["encoder.load_state_dict(torch.load('encoder512.pt', map_location=device))\n","decoder.load_state_dict(torch.load('decoder512.pt', map_location=device))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T04:26:44.604915Z","iopub.status.busy":"2022-05-28T04:26:44.604451Z","iopub.status.idle":"2022-05-28T04:26:44.616519Z","shell.execute_reply":"2022-05-28T04:26:44.6155Z","shell.execute_reply.started":"2022-05-28T04:26:44.604878Z"},"trusted":true},"outputs":[],"source":["line = val_data.sample(1)\n","line"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T04:26:55.2276Z","iopub.status.busy":"2022-05-28T04:26:55.227252Z","iopub.status.idle":"2022-05-28T04:26:55.280639Z","shell.execute_reply":"2022-05-28T04:26:55.279867Z","shell.execute_reply.started":"2022-05-28T04:26:55.227569Z"},"trusted":true},"outputs":[],"source":["s = 'أَلْفَاظُهُنَّ مُؤَنّثا'\n","\n","max_length= 50\n","bs = 1\n","\n","x = get_raw_sentence(s)\n","        \n","x_token = [vocab.index(c) for c in x]\n","        \n","x_tensor = torch.tensor(x_token, dtype=torch.int64)\n","\n","input_tensor = nn.ConstantPad1d((0, 50 - x_tensor.shape[0]), 0)(x_tensor).unsqueeze(0).to(device)\n","\n","input_tensor = input_tensor.unsqueeze(1)\n","\n","\n","encoder.eval()\n","decoder.eval()\n","\n","encoder_hidden1, encoder_hidden2, encoder_hidden3 = encoder.initHidden(bs)\n","\n","input_length = max_length\n","target_length = 4\n","\n","encoder_outputs = torch.zeros(max_length, bs, encoder.hidden_size * 2, device=device)\n","\n","loss = 0\n","\n","\n","for ei in range(input_length):\n","    encoder_output, encoder_hidden1, encoder_hidden2, encoder_hidden3 = encoder(\n","        input_tensor[:, :, ei], encoder_hidden1, encoder_hidden2, encoder_hidden3)\n","\n","    encoder_outputs[ei] = encoder_output[:, 0]\n","\n","\n","\n","#     decoder_input = torch.tensor([[SOS_token]], device=device)\n","decoder_input = torch.full((bs,1), SOS_token, device=device)\n","decoder_hidden1, decoder_hidden2, decoder_hidden3 = encoder_hidden1, encoder_hidden2, encoder_hidden3\n","\n","res = []\n","for di in range(target_length):\n","#     print(decoder_input.shape)\n","    \n","    decoder_output, decoder_hidden1, decoder_hidden2, decoder_hidden3 = decoder(\n","        decoder_input, decoder_hidden1, decoder_hidden2, decoder_hidden3)\n","\n","    topv, topi = decoder_output.topk(1)\n","    decoder_input = topi.squeeze().detach()  # detach from history as input\n","    t = decoder_input.item()\n","    decoder_input = decoder_input.view(1,-1)\n","    \n","    print(t)\n","    res.append(t)\n","\n","res = [wazn_vocab[c] for c in res]\n","print(' '.join(res))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-28T04:24:39.079512Z","iopub.status.busy":"2022-05-28T04:24:39.079159Z","iopub.status.idle":"2022-05-28T04:24:39.086478Z","shell.execute_reply":"2022-05-28T04:24:39.08567Z","shell.execute_reply.started":"2022-05-28T04:24:39.079473Z"},"trusted":true},"outputs":[],"source":["مُتْفَاعِلُنْ مُتَفَاعِلُنْ مُتْفَاعِلُنْ"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
